{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEW psth plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you just need to identify the suite2p output folder.\n",
    "Don't forget to add the NPZ file to this folder too.  \n",
    "Make sure the slashes are /, not backslashes.  \n",
    "Confirm image sampling rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get cell traces that are identified as cells by suite2p  \n",
    "Get metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### FOR TESTING ONLY #######\n",
    "\n",
    "test_mode = False\n",
    "user = \"carmen\"\n",
    "\n",
    "if test_mode and user == 'carmen':\n",
    "    output_subfolder = 'C:/Users/huynh/temp_local/carmen_feb_sce/carmen_pmth_env'\n",
    "    output_folder = 'C:/Users/huynh/temp_local/carmen_feb_sce/carmen_pmth_env/output_test'\n",
    "    suite2p_folder = 'C:/Users/huynh/temp_local/carmen_feb_sce/carmen_pmth_env/suite2p'\n",
    "    file_name = 'test_name' \n",
    "    main_excel_path = 'C:/Users/huynh/temp_local/carmen_feb_sce/carmen_pmth_env/pmth_main.xlsx'\n",
    "\n",
    "## TEST 2: fixing detrending\n",
    "if test_mode and user == \"phan\":\n",
    "    print('testing mode, user phan')\n",
    "    output_subfolder = 'C:/Users/huynh/temp_local/carmen_feb_sce/carmen_pmth_env'\n",
    "    output_folder = 'C:/Users/huynh/temp_local/carmen_feb_sce/carmen_pmth_env/output_test'\n",
    "    suite2p_folder = 'C:/Users/huynh/temp_local/carmen_feb_sce/carmen_pmth_env/suite2p_15july'\n",
    "    file_name = 'test_name' \n",
    "    main_excel_path = 'C:/Users/huynh/temp_local/carmen_feb_sce/carmen_pmth_env/pmth_main.xlsx'\n",
    "    excel_row_id = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_folder = {output_subfolders}\n",
    "# suite2p_folder = {suite2p_paths} \n",
    "imaging_sampling_rate = 10.31476027 # For carmen (2 planes on Bruker), basically same as Caro's (10.31475985). taken from XML\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Sk8gbpoTJfJV",
    "outputId": "c0f55de8-167d-4b0a-82b8-56de0827a305"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "F = np.load(os.path.join(suite2p_folder, 'F.npy'))\n",
    "iscell = np.load(os.path.join(suite2p_folder, 'iscell.npy'))\n",
    "\n",
    "print('n_cells before suite2p filter (from iscell): ', F.shape[1])\n",
    "\n",
    "traces = F[iscell[:,0]==1] # Confirmed in Cicada\n",
    "n_cells, n_frames = traces.shape\n",
    "seconds_per_frame = 1 / imaging_sampling_rate\n",
    "frames_per_second = imaging_sampling_rate\n",
    "print(\"n_cells chosen by suite2p: \", n_cells)\n",
    "print(\"n_frames: \", n_frames)\n",
    "print(\"seconds: \", n_frames/imaging_sampling_rate)\n",
    "print(\"minutes: \", n_frames/imaging_sampling_rate/60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define some preprocessing functions and apply to traces    \n",
    "This is based on JC's preprocessing for assembly detection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UHo_--YQHN4R"
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import matplotlib as plt\n",
    "import scipy\n",
    "from scipy import signal\n",
    "from scipy.stats import iqr\n",
    "from scipy.signal import find_peaks\n",
    "from scipy.signal import savgol_filter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Filters from JC's Matlab version\n",
    "\n",
    "def median_normalization(traces):\n",
    "    n_cells, n_frames = traces.shape\n",
    "    for i in range(n_cells):\n",
    "        traces[i, :] = traces[i, :] / np.median(traces[i, :])\n",
    "    return traces\n",
    "\n",
    "# def bleaching_correction(traces): ### JC's VERSION, but doesn't work as well as original CICADA version below \n",
    "#     n_cells, n_frames = traces.shape\n",
    "#     for k in range(n_cells):\n",
    "#         p0 = np.polyfit(np.arange(n_frames), traces[k, :], 3) ## originally 3\n",
    "#         traces[k, :] = (traces[k, :] / np.polyval(p0, np.arange(n_frames)))-np.mean(traces[k,:]) # PHAN: replaced this poly with detrend to better match JC's Matlab version.\n",
    "#     return traces\n",
    "\n",
    "def bleaching_correction(traces):  ### CICADA version, seems to work better than JC's matlab version.\n",
    "    n_cells, n_frames = traces.shape\n",
    "    for k in range(n_cells):\n",
    "        p0 = np.polyfit(np.arange(n_frames), traces[k, :], 3)\n",
    "        traces[k, :] = traces[k, :] / np.polyval(p0, np.arange(n_frames))\n",
    "    return traces\n",
    "\n",
    "def savitzky_golay_filt(traces): ### \n",
    "    traces = signal.savgol_filter(traces, 7, 3, axis=1)  #PHAN:  changed framelength from 5 to 7 to match JC's Matlab version.\n",
    "    return traces\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traces = bleaching_correction(traces).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "94eMKQdmMe_v"
   },
   "outputs": [],
   "source": [
    "# # Filters from JC's Matlab version\n",
    "# traces_for_sce = median_normalization(traces).copy()\n",
    "# traces_for_sce = bleaching_correction(traces_for_sce).copy()\n",
    "# traces_for_sce = savitzky_golay_filt(traces_for_sce).copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### CHECKING Bleaching correction:\n",
    "\n",
    "# array1 = traces_dff\n",
    "# array_copy = traces.copy()\n",
    "# array2 = bleaching_correction(array_copy)\n",
    "\n",
    "# n_random = 10\n",
    "# import numpy as np\n",
    "# import plotly.graph_objects as go\n",
    "# import random\n",
    "\n",
    "# ### GOOD CELL TO TEST: 145\n",
    "# selected_indices = random.sample(range(array1.shape[0]), n_random)\n",
    "# selected_indices = [145]\n",
    "\n",
    "# print(array1[selected_indices][:5])\n",
    "# print()\n",
    "# print(array2[selected_indices][:5])\n",
    "\n",
    "# fig = go.Figure()\n",
    "\n",
    "# # Add traces for the selected time series from array1\n",
    "# for idx in selected_indices:\n",
    "#     fig.add_trace(go.Scatter(y=array1[idx], mode='lines', name=f'DF/F', line=dict(color='blue', width=1)))\n",
    "\n",
    "# # Add traces for the selected time series from array2\n",
    "# for idx in selected_indices:\n",
    "#     fig.add_trace(go.Scatter(y=array2[idx]*100, mode='lines', name=f'Detrended', line=dict(color='red', width=1)))\n",
    "\n",
    "\n",
    "# # Update layout\n",
    "# fig.update_layout(\n",
    "#     title=f'Visualizing detrending vs dff normalization, cell {selected_indices}',\n",
    "#     xaxis_title='Time Points',\n",
    "#     yaxis_title='Activity',\n",
    "#     legend_title='Series',\n",
    "#     template='plotly_white'\n",
    "# )\n",
    "\n",
    "# # Show plot\n",
    "# fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detect when cells are active/spikes (SLOW)\n",
    "Definition of active taken from JC.     \n",
    "This step takes some time to process.  \n",
    "To save time, I've exported the output (saved in same suite2p folder) to be retrieved later.   \n",
    "To comment this box out (so it doesn't execute), highlight all the text (ctrl + a) and then ctrl + /."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lx77YgAyM6ai"
   },
   "outputs": [],
   "source": [
    "# # Detect Spikes traces in each trace, using JC's max of 3 threshold\n",
    "\n",
    "traces_for_sce = traces \n",
    "\n",
    "n_cells, n_frames = traces_for_sce.shape\n",
    "\n",
    "window_size = int(4 * imaging_sampling_rate)   # 4 seconds long window as Arnaud was doing with 40 in 10Hz recording\n",
    "print(f\"Window size is {window_size} frames\")\n",
    "\n",
    "minithreshold = 0.2\n",
    "MinPeakDistance = 3\n",
    "\n",
    "WinActive = [] # WinActive = np.where(speed > 1)[0]\n",
    "\n",
    "th_cic = []\n",
    "activity_tmp_all_cells = [[] for i in range(n_cells)]\n",
    "\n",
    "if test_mode:\n",
    "    activity_tmp_all_cells_array = np.load(os.path.join(output_folder, 'activity_tmp_all_cells.npy'), allow_pickle=True)\n",
    "    activity_tmp_all_cells = list(activity_tmp_all_cells_array)\n",
    "else:\n",
    "\n",
    "    for i in range(n_cells):\n",
    "        activity_tmp_all_cells = [None] * n_cells\n",
    "        for i in range(n_cells):\n",
    "            th_i = max([3 * iqr(traces_for_sce[i]), 3 * np.std(traces_for_sce[i]), minithreshold])\n",
    "            th_cic.append(th_i)\n",
    "            peaks, properties = find_peaks(traces_for_sce[i], prominence=th_i, distance=MinPeakDistance)\n",
    "            valeurs_identiques = np.intersect1d(peaks, WinActive)\n",
    "            locs_sans_ide = np.setdiff1d(peaks, valeurs_identiques)\n",
    "            activity_tmp_all_cells[i] = locs_sans_ide\n",
    "\n",
    "    #convert list of arrays to array of arrays in order to save to npy.file\n",
    "    activity_tmp_all_cells_array = np.array(activity_tmp_all_cells, dtype=object)\n",
    "\n",
    "    np.save(os.path.join(output_folder, 'activity_tmp_all_cells.npy'), activity_tmp_all_cells_array)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If output is already saved, here you can retrieve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TiguGRDkO4U-"
   },
   "outputs": [],
   "source": [
    "n_spikes = sum(len(arr) for arr in activity_tmp_all_cells)\n",
    "spikes_per_sec = n_spikes/(n_frames/imaging_sampling_rate)\n",
    "spikes_per_sec_per_cell = spikes_per_sec/n_cells\n",
    "\n",
    "# print(\"n_spikes:\", n_spikes)\n",
    "# print(\"spikes_per_min_per_cell:\", spikes_per_min_per_cell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we find timepoints where there are multiple active cells (over 10).  \n",
    "This cell's output is the main info you're looking for.  The rest are just plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_raster(activity_tmp_all_cells):\n",
    "    raster = np.zeros((n_cells, n_frames))\n",
    "    for i in range(n_cells):\n",
    "        raster[i, activity_tmp_all_cells[i]] = 1\n",
    "    return raster\n",
    "\n",
    "def get_sce_threshold(rasterdur, n_surrogates=100, percentile=99, verbose=False): # ROBIN's Paper:  300 n_surr, 99th percentile.  Cicada default is 100, 95.  Changed cicada default per Rosa (5 June meeting)\n",
    "    [n_cells, n_frames] = rasterdur.shape\n",
    "    if verbose:\n",
    "        print(f\"Starting to obtain the {n_surrogates} rolled rasters\")\n",
    "    rnd_rasters = np.zeros((n_cells, n_frames, n_surrogates))\n",
    "    for surrogate in range(n_surrogates):\n",
    "        for cell in range(n_cells):\n",
    "            rnd_rasters[cell, :, surrogate] = np.roll(rasterdur[cell, :], np.random.randint(1, n_frames))\n",
    "\n",
    "    rnd_sum_cells = np.sum(rnd_rasters, axis=0)\n",
    "    n_values = n_frames * n_surrogates\n",
    "    rnd_sum_cells_vector = np.reshape(rnd_sum_cells, (n_values,))\n",
    "    cell_threshold = np.percentile(rnd_sum_cells_vector, percentile)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Threshold obtain for SCE detection with {percentile} percentile: {cell_threshold} cells, \"\n",
    "              f\"representing {np.round((cell_threshold / n_cells) * 100, decimals=1)} % of the detected cells\")\n",
    "\n",
    "    return rnd_rasters, cell_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V8kvJ_FZP533",
    "outputId": "477b443e-2d83-4776-a6c9-92ca384cd566"
   },
   "outputs": [],
   "source": [
    "from scipy import signal, stats\n",
    "\n",
    "matrix, sce_n_cells_threshold = get_sce_threshold(convert_to_raster(activity_tmp_all_cells))\n",
    "sce_manual_threshold = 5 \n",
    "sce_percent_threshold = int(.03*n_cells)\n",
    "sce_min_distance_sec = 1\n",
    "\n",
    "sce_min_distance_frames = sce_min_distance_sec*frames_per_second # before, it was 4 (default from CICADA), but changed to 3 (10 june) based on JC's assembly_SCE6.m on github\n",
    "\n",
    "print(\"95% percentile of surrogate: \", sce_n_cells_threshold)  ###  \n",
    "print(\"3% of cells: \", sce_percent_threshold)  ### lowering to 3% based on Carmen convo (10 JUNE)\n",
    "print(\"manual minimum: 5\")\n",
    "n_synchronous_frames = 2\n",
    "\n",
    "# sce_n_cells_threshold = max(sce_calc_threshold, sce_percent_threshold,sce_manual_threshold) ### 5% of cells removed per Rosa (April 25)\n",
    "# sce_n_cells_threshold = sce_calc_threshold.copy() ### USE ONLY sce_calc_threhold, at 99% and 1 sec min distance, per rosa 12 june\n",
    "\n",
    "print(\"sce_n_cells_threshold: \", sce_n_cells_threshold)\n",
    "\n",
    "\n",
    "raster = np.zeros((n_cells, n_frames))\n",
    "\n",
    "for i in range(n_cells):\n",
    "    raster[i, activity_tmp_all_cells[i]] = 1\n",
    "\n",
    "# sum activity over n consecutive frames\n",
    "sum_activity = np.zeros(n_frames - n_synchronous_frames)\n",
    "for i in range(n_frames - n_synchronous_frames):\n",
    "    sum_activity[i] = np.sum(np.amax(raster[:, np.arange(i, i + n_synchronous_frames)], axis=1))\n",
    "\n",
    "# select synchronous calcium events\n",
    "sce_loc = signal.find_peaks(sum_activity, height=sce_n_cells_threshold, distance=sce_min_distance_frames)[0]\n",
    "n_sce = len(sce_loc)\n",
    "sce_trace = np.array([np.array([1 if i in sce_loc else 0 for i in range(n_frames)])])\n",
    "\n",
    "sce_per_sec = n_sce/(n_frames/imaging_sampling_rate)\n",
    "print(\"SCEs detected: \", n_sce)\n",
    "print(\"SECs per second: \", n_sce/(n_frames/imaging_sampling_rate), \"(range should be .1 to .5)\")\n",
    "print(\"SECs per minute: \", n_sce/(n_frames/imaging_sampling_rate/60))\n",
    "\n",
    "\n",
    "### comparing n_sce to 3% of cells:\n",
    "# select synchronous calcium events\n",
    "sce_loc_percent = signal.find_peaks(sum_activity, height=sce_percent_threshold, distance=sce_min_distance_frames)[0]\n",
    "n_sce_percent = len(sce_loc_percent)\n",
    "print(\"n_sce_percent: \", n_sce_percent)\n",
    "sce_loc_manual = signal.find_peaks(sum_activity, height=sce_manual_threshold, distance=sce_min_distance_frames)[0]\n",
    "n_sce_manual = len(sce_loc_manual)\n",
    "print(\"n_sce_manual: \", n_sce_manual)\n",
    "\n",
    "### create cells vs sce matrix\n",
    "# sce_cells_matrix = np.zeros((n_cells, n_sce))\n",
    "# for i in range(n_sce):\n",
    "#     sce_cells_matrix[:, i] = np.amax(raster[:, np.arange(np.max((sce_loc[i]-1, 0)),\n",
    "#                                                             np.min((sce_loc[i]+2, n_frames)))], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before plotting, I use TSNE to reorder the cells.  \n",
    "(I don't think it helps much)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RDejj_ESQOlT"
   },
   "outputs": [],
   "source": [
    "# from sklearn.manifold import TSNE\n",
    "\n",
    "# #### APPLY TSNE to reorder sce matrix\n",
    "\n",
    "# # Assuming Tr1b is your data, replace it with your actual data\n",
    "# Tr1b = sce_cells_matrix.copy()\n",
    "\n",
    "# # Set TSNE parameters\n",
    "# tsne = TSNE(\n",
    "#     n_components=1, # how many dimensions to reduce to, 1 is just a list, 2 for 2D, 3 for 3D, etc.\n",
    "#     # n_iter=500,\n",
    "#     # perplexity=1, #\n",
    "#     # early_exaggeration=500,\n",
    "#     # metric=\"correlation\",\n",
    "#     # method=\"barnes_hut\",\n",
    "#     random_state=42  # Set a random state for reproducibility\n",
    "# )\n",
    "\n",
    "# # Fit TSNE and get the transformed data\n",
    "# TSNE_result = tsne.fit_transform(Tr1b)\n",
    "\n",
    "# # Get sorted indices\n",
    "# index = np.argsort(TSNE_result[:, 0])\n",
    "\n",
    "# # Sort the TSNE result based on the sorted indices\n",
    "# B = TSNE_result[index]\n",
    "# # Now B and index contain the equivalent results to the MATLAB code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.cluster import DBSCAN\n",
    "\n",
    "# dbscan = DBSCAN()\n",
    "\n",
    "# tsne = TSNE(\n",
    "#     n_components=3, # how many dimensions to reduce to, 1 is just a list, 2 for 2D, 3 for 3D, etc.\n",
    "#     # n_iter=500,\n",
    "#     # perplexity=1, #\n",
    "#     # early_exaggeration=500,\n",
    "#     # metric=\"correlation\",\n",
    "#     # method=\"barnes_hut\",\n",
    "#     random_state=42  # Set a random state for reproducibility\n",
    "# )\n",
    "\n",
    "# # Fit TSNE and get the transformed data\n",
    "# TSNE_result = tsne.fit_transform(Tr1b)\n",
    "# labels = dbscan.fit_predict(TSNE_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make raster plot (with and without TSNE cell ordering).  \n",
    "This plot shows just the timepoints where there are at least ten cells active."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "# # Create a figure and set the size\n",
    "# plt.figure(figsize=(20, 10))\n",
    "\n",
    "# # Define the number of rows and columns for the subplots\n",
    "# rows = 3\n",
    "# cols = 1\n",
    "\n",
    "# # Plot the first heatmap (top one)\n",
    "# plt.subplot(rows, cols, 1)\n",
    "# sns.heatmap(sce_cells_matrix, cmap='gray_r', cbar=True)\n",
    "\n",
    "# # Add title and axis labels for the top heatmap\n",
    "# plt.title('SCE cells matrix (unsorted)')\n",
    "# plt.xlabel('SCE (frame)')\n",
    "# plt.ylabel('Cell ID')\n",
    "\n",
    "# # Plot the second heatmap (bottom one)\n",
    "# plt.subplot(rows, cols, 2)\n",
    "# sns.heatmap(sce_cells_matrix[index], cmap='gray_r', cbar=True)\n",
    "\n",
    "# plt.subplot(rows, cols, 3)\n",
    "# sns.heatmap(sce_cells_matrix[index1], cmap='gray_r', cbar=True)\n",
    "\n",
    "# # Add title and axis labels for the bottom heatmap\n",
    "# plt.title('SCE cells matrix (sorted)')\n",
    "# plt.xlabel('SCE (frame)')\n",
    "# plt.ylabel('Cell ID')\n",
    "\n",
    "# # Adjust layout to prevent overlap\n",
    "# plt.tight_layout()\n",
    "\n",
    "# # Show the plot\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create heatmap of all cell activities across all timepoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get behavior annotation from NPZ file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from numpy import load\n",
    "# import os\n",
    "# import numpy as np \n",
    "# suite2p_folder = 'D:/phan/Plane0/suite2p_231011_231023_Plane0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## GET EPOCH TIMES FROM NPZ #### (In cicada, it takes in timestamps in seconds (probably from NPZ) and converts to frames \n",
    "from numpy import load\n",
    "\n",
    "\n",
    "files = os.listdir(suite2p_folder)\n",
    "npz_file = [file for file in files if file.endswith('.npz')][0]\n",
    "npz_data = load(os.path.join(suite2p_folder, npz_file),allow_pickle=True)\n",
    "\n",
    "data_types = npz_data.files #['Twitches', 'Startles', 'Complex_Movements']\n",
    "print(\"data_types before: \", data_types)\n",
    "twitch_string = next(s for s in data_types if 'twitch' in s.lower())\n",
    "startle_string = next(s for s in data_types if 'start' in s.lower())\n",
    "complex_string = next(s for s in data_types if 'complex' in s.lower())\n",
    "\n",
    "movement_types_all = [twitch_string,startle_string,complex_string ] # remove MotCorr Auto Detection\n",
    "movement_types_no_complex = [twitch_string, startle_string]\n",
    "n_movement_types = len(movement_types_all)\n",
    "\n",
    "twitches = (npz_data[twitch_string][0]*imaging_sampling_rate).astype(int)\n",
    "startles = (npz_data[startle_string][0]*imaging_sampling_rate).astype(int)\n",
    "complex = (npz_data[complex_string][0]*imaging_sampling_rate).astype(int)\n",
    "\n",
    "movements_frame_list = [twitches, startles, complex]\n",
    "\n",
    "lst = movement_types_all # movement_types_no_complex\n",
    "# for item in lst:\n",
    "#     print(item)\n",
    "#     print(data[item][0])\n",
    "#     # print((data[item][0]*imaging_sampling_rate).astype(int))\n",
    "#     # movements = np.concatenate((movements, data[item][0]))\n",
    "# print(\"\")\n",
    "\n",
    "movements_frame = []\n",
    "movements_sec = []\n",
    "for item in lst:\n",
    "    sec_temp = npz_data[item][0]\n",
    "    movements_sec.extend(sec_temp)\n",
    "    \n",
    "    frame_temp = (npz_data[item][0]*imaging_sampling_rate).astype(int)\n",
    "    movements_frame.extend(frame_temp)\n",
    "    # movement_frames = np.concatenate(movement_frames, frame_temp)\n",
    "\n",
    "#Convert to frames\n",
    "movements_sec = np.array(movements_sec)\n",
    "movements_frame = np.array(movements_frame, dtype=np.int32)\n",
    "\n",
    "# movements = np.sort(movements)\n",
    "print(\"\")\n",
    "\n",
    "movement_count = len(movements_frame)\n",
    "movement_per_sec = movement_count/(n_frames/imaging_sampling_rate)\n",
    "movement_per_min = movement_count/(n_frames/imaging_sampling_rate/60)\n",
    "print(\"count: \", len(movements_frame))\n",
    "print(\"movement_per_sec: \", movement_per_sec)\n",
    "print(\"movement_per_min: \", movement_per_min)\n",
    "# print(movements_sec)\n",
    "# print(np.sort(movements_frame))\n",
    "\n",
    "fake_movements_frame = np.random.randint(0, n_frames+1, len(movements_frame))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_sce_to_movement(sce, movement, buffer):\n",
    "    sce_matched = []\n",
    "    sce_idx_matched = []\n",
    "    for i, a in enumerate(sce):\n",
    "        for j, b in enumerate(movement):\n",
    "            if abs(a - b) <= buffer:\n",
    "                if a not in sce_matched:\n",
    "                    sce_matched.append(a)\n",
    "                    # sce_idx_matched.append(i)\n",
    "                break  # Move to the next element in A\n",
    "    return np.array(sce_matched)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can adjust the brightness of the heatmap by changing the max_bright and min_bright values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bjxqaCC1RmXm",
    "outputId": "eeb0b28f-9eea-4eac-cdf2-930b1c109415"
   },
   "outputs": [],
   "source": [
    "# Try different normalization methods to make rasterplot more visible\n",
    "\n",
    "from scipy import stats\n",
    "traces_zscore = stats.zscore(traces, axis=1)\n",
    "\n",
    "# min max normalize traces at each row\n",
    "traces_min = np.min(traces, axis=1)\n",
    "traces_max = np.max(traces, axis=1)\n",
    "traces_minmax = (traces - traces_min[:, np.newaxis]) / (traces_max - traces_min)[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To save the interactive heatmap in your suite2p folder, set \"save_to_html\" = True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from scipy.stats import zscore\n",
    "\n",
    "export_heatmap_sce = True # check folder path \n",
    "plot_traces = traces_zscore # traces_zscore # traces_minmax # traces_zscore\n",
    "order = range(0,n_cells) # index # isort # tsne_index\n",
    "\n",
    "# plot trace average.  \n",
    "traces_sum = np.sum(traces, axis=0)\n",
    "traces_avg = traces_sum / n_cells\n",
    "traces_line = traces_avg # for plotting\n",
    "traces_line = (traces_line - np.min(traces_line))*1\n",
    "# Assuming traces_minmax, index, movements, imaging_sampling_rate, traces, and sce_loc are your data arrays\n",
    "\n",
    "# Calculate the maximum amplitude for sum_activity and line_trace\n",
    "max_amplitude = 0.75 * n_cells\n",
    "# max_sum_activity = np.max(sum_activity_full) * 10\n",
    "max_traces_line = np.max(traces_line)\n",
    "# sum_activity_scaled = (sum_activity_full / max_sum_activity) * max_amplitude * magnification_factor\n",
    "traces_line_scaled = (traces_line / max_traces_line) * max_amplitude \n",
    "\n",
    "sce_matched = match_sce_to_movement(sce_loc, movements_frame, 5)\n",
    "sce_not_matched = np.setdiff1d(sce_loc, sce_matched)\n",
    "# print(sce_matched)\n",
    "\n",
    "# Create a heatmap\n",
    "heatmap_trace = go.Heatmap(\n",
    "    z=plot_traces[order],\n",
    "    # z = zscore(traces[order], axis=1)\n",
    "    colorscale= 'Reds', # gray_r = reverse color\n",
    "    showscale=True,\n",
    "    name='Heatmap',  # Name for the legend\n",
    "    zmin=0,  # zmin, Set a suitable minimum value for the colormap\n",
    "    zmax=1.96, # 1.96 is one std, what you want for zscore\n",
    "    colorbar=dict(len=0.4)  # Set the length of the color scale to 40% of the original length\n",
    ")\n",
    "\n",
    "# Mark SCEs\n",
    "sce_matched = go.Scatter(\n",
    "    x=sce_matched,\n",
    "    y=[n_cells - 1] * n_frames,  \n",
    "    # y=[0] * len(sce_loc),\n",
    "    mode='markers',\n",
    "    marker=dict(color='green', symbol='triangle-down', size=10),\n",
    "    name=f'{len(sce_matched)} matched SCEs',  # Name for the legend\n",
    ")\n",
    "\n",
    "# Mark SCEs\n",
    "sce_not_matched = go.Scatter(\n",
    "    x=sce_not_matched,\n",
    "    y=[n_cells - 1] * n_frames,  \n",
    "    # y=[0] * len(sce_loc),\n",
    "    mode='markers',\n",
    "    marker=dict(color='red', symbol='triangle-down', size=10),\n",
    "    name=f'{len(sce_not_matched)} unmatched SCEs',  # Name for the legend\n",
    ")\n",
    "\n",
    "# Mark behavior\n",
    "scatter_twitches = go.Scatter(\n",
    "    x=twitches,\n",
    "    y=[0] * len(movements_frame),\n",
    "    mode='markers',\n",
    "    marker=dict(color='red', symbol='triangle-up', size=10),\n",
    "    name=f'{len(twitches)} twitches',  # Name for the legend\n",
    ")\n",
    "\n",
    "# Mark startles\n",
    "scatter_startles = go.Scatter(\n",
    "    x=startles,\n",
    "    y=[0] * len(movements_frame),\n",
    "    mode='markers',\n",
    "    marker=dict(color='blue', symbol='triangle-up', size=10),\n",
    "    name=f'{len(startles)} startles',  # Name for the legend\n",
    ")\n",
    "\n",
    "# Mark complex movements\n",
    "scatter_complex = go.Scatter(\n",
    "    x=complex,\n",
    "    y=[0] * len(movements_frame),\n",
    "    mode='markers',\n",
    "    marker=dict(color='green', symbol='triangle-up', size=10),\n",
    "    name=f'{len(complex)} complex',  # Name for the legend\n",
    ")\n",
    "\n",
    "# # Draw line of average \n",
    "# line_trace = go.Scatter(\n",
    "#     x=np.arange(len(traces_line)),\n",
    "#     y=traces_line_scaled,\n",
    "#     mode='lines',\n",
    "#     line=dict(color='red', width=.5),  # Set line color and width\n",
    "#     name='traces_avg',  # Name for the legend\n",
    "# )\n",
    "\n",
    "# # Draw line of spike count \n",
    "# sum_activity = go.Scatter(\n",
    "#     x=np.arange(len(traces_line)),\n",
    "#     y=sum_activity_full*10,\n",
    "#     mode='lines',\n",
    "#     line=dict(color='red', width=2),  # Set line color and width\n",
    "#     name='sum_activity',  # Name for the legend\n",
    "# )\n",
    "\n",
    "# Create layout with aspect ratio adjustment\n",
    "layout = go.Layout(\n",
    "    title=(f'Cell activity with SCE and behavior annotations<br>{file_name}'),\n",
    "    xaxis=dict(title='Time (frame)'),\n",
    "    yaxis=dict(title='Cell ID'),\n",
    "    height=700,  # Set the height of the plot\n",
    "    width=1300,  # Set the width of the plot\n",
    ")\n",
    "\n",
    "# Create figure with adjusted legend order\n",
    "# fig = go.Figure(data=[heatmap_trace, sce_marker, scatter_twitches, scatter_startles, scatter_complex], layout=layout)\n",
    "\n",
    "fig = go.Figure(data=[heatmap_trace, sce_matched, sce_not_matched, scatter_twitches, scatter_startles, scatter_complex], layout=layout)\n",
    "\n",
    "# Show the plot\n",
    "fig.show()\n",
    "\n",
    "if export_heatmap_sce:\n",
    "    fig.write_html(os.path.join(output_folder, \"heatmap_sce.html\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create PSTH plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_dff (traces):\n",
    "    dff = []\n",
    "    for trace in traces:\n",
    "        dff_trace = (trace - np.median(trace))*100 / np.median(trace)\n",
    "        dff.append(np.array(dff_trace))\n",
    "    return np.vstack(dff)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### DEBLEACHING IS ALREADY A NORMALIZING, instead, we'll zscore to make values closer and make heatmap more pretty.\n",
    "\n",
    "traces_dff = convert_to_dff(traces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calculate MULTIPLE ave spikes from MULTIPLE traces\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "traces_to_plot = traces_dff\n",
    "# traces_to_plot = traces\n",
    "\n",
    "range_value=104\n",
    "lower_percent = 5 \n",
    "upper_percent = 95\n",
    "\n",
    "post_analysis_length = 50 # from center\n",
    "pre_analysis_length = 25 # from center \n",
    "\n",
    "avg_spike_all_traces = []\n",
    "trace_length = len(traces[0])  # Assuming all traces have the same length\n",
    "positive_trace_index = []\n",
    "neutral_trace_index = []\n",
    "negative_trace_index = []\n",
    "max_spike_all_traces = []\n",
    "min_spike_all_traces = []\n",
    "margins_to_threshold = []\n",
    "t_values = []\n",
    "p_values = []\n",
    "subsegment_avg = []\n",
    "subsegment_max = []\n",
    "subsegment_min = []\n",
    "pre_avgs = []\n",
    "pre_stds = []\n",
    "post_z_max = []\n",
    "post_z_min = []\n",
    "\n",
    "for trace in traces_to_plot:\n",
    "    spikes_per_trace = []\n",
    "\n",
    "    upper_threshold = np.percentile(traces_to_plot, upper_percent)\n",
    "    lower_threshold = np.percentile(traces_to_plot, lower_percent)\n",
    "    \n",
    "    # upper_threshold = np.median(trace)+np.std(trace)\n",
    "    # lower_threshold = np.median(trace)-np.std(trace)\n",
    "    \n",
    "    # print(\"upper_threshold:\", upper_threshold)\n",
    "    # print(\"lower_threshold: \", lower_threshold)\n",
    "    for frame_index in movements_frame:\n",
    "        # Calculate the start and end index for the segment\n",
    "        beg_frame = max(frame_index - range_value, 0)  # Ensure beg_frame is not negative\n",
    "        end_frame = min(frame_index + range_value + 1, trace_length)  # Ensure end_frame is within trace_length\n",
    "        \n",
    "        # Extract the segment from the time series and pad with zeros if necessary\n",
    "        segment = trace[beg_frame:end_frame]\n",
    "\n",
    "        if len(segment) < 2 * range_value + 1:\n",
    "            pad_length = 2 * range_value + 1 - len(segment)\n",
    "            segment = list(segment)\n",
    "            segment.extend([0] * pad_length)\n",
    "            segment = np.array(segment)          \n",
    "        \n",
    "        spikes_per_trace.append(segment)\n",
    "\n",
    "\n",
    "\n",
    "    # print(len(segments))\n",
    "    # Compute the average of the segments for the current trace\n",
    "    avg_spike_per_trace = [sum(values) / len(spikes_per_trace) for values in zip(*spikes_per_trace)]\n",
    "    \n",
    "    subsegment_avg.append(np.average(avg_spike_per_trace[range_value:range_value+post_analysis_length]))\n",
    "    subsegment_max.append(np.max(avg_spike_per_trace[range_value:range_value+post_analysis_length]))\n",
    "    subsegment_min.append(np.min(avg_spike_per_trace[range_value:range_value+post_analysis_length]))\n",
    "\n",
    "    # Perform t-test between pre and post movement \n",
    "\n",
    "    # pre_avg_spike_per_trace = avg_spike_per_trace[:range_value]\n",
    "    # post_avg_spike_per_trace = avg_spike_per_trace[range_value+1:]\n",
    "\n",
    "    # pre_avg_spike_per_trace = avg_spike_per_trace[range_value-pre_analysis_length : range_value]\n",
    "\n",
    "    pre_avg_spike_per_trace = avg_spike_per_trace[: range_value] ### WHOLE WINDOW (NOT USING pre_analysis_length)\n",
    "    post_avg_spike_per_trace = avg_spike_per_trace[range_value: range_value+post_analysis_length]\n",
    "\n",
    "    t_stat, p_value = ttest_ind(post_avg_spike_per_trace, pre_avg_spike_per_trace)\n",
    "    t_values.append(t_stat)\n",
    "    p_values.append(p_value)\n",
    "\n",
    "    max_spike_all_traces.append(np.max(avg_spike_per_trace))                                    \n",
    "    min_spike_all_traces.append(np.min(avg_spike_per_trace))\n",
    "    \n",
    "    margins_to_threshold.append(upper_threshold-np.max(avg_spike_per_trace))\n",
    "\n",
    "    # print(\"max of avg_spike_per_trace: \",np.max(avg_spike_per_trace)\n",
    "    if np.max(avg_spike_per_trace) > upper_threshold:\n",
    "            positive_trace_index.append(trace)\n",
    "    if np.min(avg_spike_per_trace) < lower_threshold:\n",
    "            negative_trace_index.append(trace)\n",
    "    if np.min(avg_spike_per_trace) >= lower_threshold and np.max(avg_spike_per_trace) <= upper_threshold:\n",
    "            neutral_trace_index.append(trace)\n",
    "    \n",
    "    avg_spike_all_traces.append(avg_spike_per_trace)\n",
    "\n",
    "    #### APPLYING ARTEM'S Z SCORE METHOD\n",
    "    pre_avg = np.average(avg_spike_per_trace[range_value-pre_analysis_length : range_value]) ### Pre window is used, NOT WHOLE WINDOW.\n",
    "    pre_avgs.append(pre_avg)\n",
    "    pre_std = np.std(avg_spike_per_trace[range_value-pre_analysis_length : range_value]) ### Pre window is used, NOT WHOLE WINDOW.\n",
    "    pre_stds.append(pre_std)\n",
    "    # print(\"post_avg_spike_per_trace: \", type(post_avg_spike_per_trace))\n",
    "    post_z = (np.array(post_avg_spike_per_trace) - pre_avg)/pre_std\n",
    "    post_z_max.append(np.max(post_z[:post_analysis_length]))\n",
    "    post_z_min.append(np.min(post_z[:post_analysis_length]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### Plotting the histogram\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.hist(post_z_max, bins=30, edgecolor='black')\n",
    "# plt.title('Histogram of post_z_max')\n",
    "# plt.xlabel('post_z_max values')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### Plotting the histogram\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.hist(post_z_min, bins=30, edgecolor='black')\n",
    "# plt.title('Histogram of post_z_min')\n",
    "# plt.xlabel('post_z_min values')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PLOTTING ALL TRACES\n",
    "\n",
    "export_movement_average = True # check folder path \n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "x_values = np.linspace(-range_value, range_value, len(avg_spike_all_traces[0]))\n",
    "\n",
    "# Plot individual traces\n",
    "for spike in avg_spike_all_traces:\n",
    "    fig.add_trace(go.Scatter(x=x_values, y=spike, mode='lines', name='Individual Trace', line=dict(color='lightgrey', width=1), showlegend=False))\n",
    "\n",
    "# Plot mean trace\n",
    "mean_line = np.mean(avg_spike_all_traces, axis=0)\n",
    "fig.add_trace(go.Scatter(x=x_values, y=mean_line, mode='lines', name='Mean Trace', line=dict(color='black', width=3), showlegend=False))\n",
    "\n",
    "# # Add horizontal lines to the figure\n",
    "# for line in horizontal_lines:\n",
    "#     fig.add_trace(line)\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title=(f'PMTH traces and average<br>{file_name}'), \n",
    "    xaxis_title='Time (frame)', \n",
    "    yaxis_title='df/f %', \n",
    "    xaxis=dict(range=[-range_value, range_value]),  # Set x-axis range from -104 to 104\n",
    "    width=600, \n",
    "    height=400  # Set the height to 400\n",
    ")\n",
    "# Show plot\n",
    "fig.show()\n",
    "\n",
    "if export_movement_average:\n",
    "    fig.write_html(os.path.join(output_folder, \"pmth_average.html\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "threshold_value = 0.05 # for p values\n",
    "\n",
    "# Get indices of values above and below 0.05\n",
    "trace_id_sig = [i for i, p_val in enumerate(p_values) if p_val <= threshold_value]\n",
    "trace_id_insig = [i for i, p_val in enumerate(p_values) if p_val > threshold_value]\n",
    "\n",
    "# Sort only certain indices based on the condition\n",
    "trace_id_sorted_sig = sorted(trace_id_sig, key=lambda idx: t_values[idx], reverse=True)\n",
    "trace_id_sorted_insig = sorted(trace_id_insig, key=lambda idx: t_values[idx], reverse=True)\n",
    "\n",
    "trace_id_sorted_sig_pos = []\n",
    "trace_id_sorted_sig_neg = []\n",
    "\n",
    "for i, idx in enumerate(trace_id_sorted_sig):\n",
    "    if t_values[idx] >= 0:\n",
    "        trace_id_sorted_sig_pos.append(idx)\n",
    "        # print('pos ', i, ' ', t_values[idx])\n",
    "    else:\n",
    "        trace_id_sorted_sig_neg.append(idx)\n",
    "        # print('neg ',i,' ', t_values[idx])\n",
    "\n",
    "# Print the sorted indices\n",
    "# print(\"Sorted indices sig:\", trace_id_sorted_sig)\n",
    "# print(\"Sorted indices insig:\", trace_id_sorted_insig)\n",
    "\n",
    "# print(len(trace_id_sorted_sig), ' ', len(trace_id_sorted_insig))\n",
    "# print(len(trace_id_sorted_sig)+len(trace_id_sorted_insig))\n",
    "print(len(trace_id_sorted_sig_pos), ' ', len(trace_id_sorted_insig), ' ',len(trace_id_sorted_sig_neg) )\n",
    "print(len(trace_id_sorted_sig_pos)+len(trace_id_sorted_sig_neg)+len(trace_id_sorted_insig))\n",
    "\n",
    "######## CARMEN VERSION 2, USING JUST THE AVERAGE OF THE 20 frames after movement ########\n",
    "\n",
    "# Sort only certain indices based on the condition\n",
    "\n",
    "# subsegment_order = np.array(subsegment_max)\n",
    "\n",
    "\n",
    "# trace_id_sorted_sig_2 = list(trace_id_sig[np.argsort(subsegment_order[trace_id_sig])[::-1]])\n",
    "# trace_id_sorted_insig_2 = list(trace_id_insig[np.argsort(subsegment_order[trace_id_insig])[::-1]])\n",
    "\n",
    "# trace_id_sorted_sig_pos_2 = []\n",
    "# trace_id_sorted_sig_neg_2 = []\n",
    "\n",
    "# for i, idx in enumerate(trace_id_sorted_sig_2):\n",
    "#     if subsegment_order[idx] >= 0:\n",
    "#         trace_id_sorted_sig_pos_2.append(idx)\n",
    "#         # print('pos ', i, ' ', t_values[idx])\n",
    "#     else:\n",
    "#         trace_id_sorted_sig_neg_2.append(idx)\n",
    "#         # print('neg ',i,' ', t_values[idx])\n",
    "\n",
    "######## CARMEN VERSION 3, AFTER SEPERATING INTO THREE (t pos, t neg, and t insgig), THEN SORTING MAX AMP OF THE 20 frames after movement ########\n",
    "\n",
    "# subsegment_order = np.array(subsegment_max)\n",
    "\n",
    "# trace_id_sorted_sig_pos_2 = list(trace_id_sig[np.argsort(subsegment_order[trace_id_sorted_sig_pos])[::-1]])\n",
    "# trace_id_sorted_sig_neg_2 = list(trace_id_sig[np.argsort(subsegment_order[trace_id_sorted_sig_neg])[::-1]])\n",
    "# trace_id_sorted_insig_2 = list(trace_id_insig[np.argsort(subsegment_order[trace_id_insig])[::-1]])\n",
    "\n",
    "######### ARTERM VERSION OF SORTING \n",
    "subsegment_order = np.array(subsegment_max)\n",
    "\n",
    "upper_z_threshold = 1.96 \n",
    "lower_z_threshold = -1.65\n",
    "\n",
    "trace_id_sig_pos = np.array([i for i in range(len(post_z_max)) if post_z_max[i] >= upper_z_threshold and post_z_min[i] >= lower_z_threshold])\n",
    "trace_id_sig_neg = np.array([i for i in range(len(post_z_max)) if post_z_max[i] <= upper_z_threshold and post_z_min[i] <= lower_z_threshold])\n",
    "trace_id_insig = np.array([i for i in range(len(post_z_max)) if (post_z_max[i] < upper_z_threshold and post_z_min[i] > lower_z_threshold) or (post_z_max[i] > upper_z_threshold and post_z_min[i] < lower_z_threshold)])\n",
    "\n",
    "if trace_id_sig_neg.size > 0:  # Check if trace_id_sig_neg is not empty\n",
    "    trace_id_sorted_sig_neg = list(trace_id_sig_neg[np.argsort(subsegment_order[trace_id_sig_neg])[::-1]])\n",
    "else:\n",
    "    trace_id_sorted_sig_neg = []  # Assign an empty list if trace_id_sig_neg is empty\n",
    "\n",
    "if trace_id_sig_pos.size > 0:  # Check if trace_id_sig_pos is not empty\n",
    "    trace_id_sorted_sig_pos = list(trace_id_sig_pos[np.argsort(subsegment_order[trace_id_sig_pos])[::-1]])\n",
    "else:\n",
    "    trace_id_sorted_sig_pos = []  # Assign an empty list if trace_id_sig_pos is empty\n",
    "\n",
    "if trace_id_insig.size > 0:  # Check if trace_id_insig is not empty\n",
    "    trace_id_sorted_insig = list(trace_id_insig[np.argsort(subsegment_order[trace_id_insig])[::-1]])\n",
    "else:\n",
    "    trace_id_sorted_insig = []  # Assign an empty list if trace_id_insig is empty\n",
    "\n",
    "\n",
    "print(len(trace_id_sorted_sig_pos), ' ', len(trace_id_sorted_sig_neg), ' ',len(trace_id_sorted_insig) )\n",
    "print(len(trace_id_sorted_sig_pos)+len(trace_id_sorted_sig_neg)+len(trace_id_sorted_insig))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### HISTOGRAM OF P VALUES\n",
    "# histogram = go.Histogram(\n",
    "#     x=p_values,\n",
    "#     nbinsx=50,  # Number of bins\n",
    "#     marker=dict(color='blue'),  # Color of the bars\n",
    "#     opacity=0.75  # Opacity of the bars\n",
    "# )\n",
    "# layout = go.Layout(\n",
    "#     title='Histogram of p-values',\n",
    "#     xaxis=dict(title='p-value'),\n",
    "#     yaxis=dict(title='Frequency'),\n",
    "#     height=750,\n",
    "#     width=750,\n",
    "#     shapes=[\n",
    "#         # Vertical line at the threshold value\n",
    "#         dict(\n",
    "#             type=\"line\",\n",
    "#             xref=\"x\",\n",
    "#             yref=\"paper\",\n",
    "#             x0=threshold_value,\n",
    "#             y0=0,\n",
    "#             x1=threshold_value,\n",
    "#             y1=1,\n",
    "#             line=dict(color=\"red\", width=2)\n",
    "#         )\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# # Create the figure\n",
    "# fig = go.Figure(data=[histogram], layout=layout)\n",
    "\n",
    "# # Show the figure\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "\n",
    "export_movement_heatmap = True\n",
    "\n",
    "sig_pos_count = len(trace_id_sorted_sig_pos)\n",
    "sig_neg_count = len(trace_id_sorted_sig_neg)\n",
    "insig_count = len(trace_id_sorted_insig)\n",
    "\n",
    "total_count = sig_pos_count + sig_neg_count + insig_count\n",
    "\n",
    "\n",
    "# plot_traces = np.array(avg_spike_all_traces)\n",
    "plot_traces = stats.zscore(avg_spike_all_traces, axis=1)\n",
    "\n",
    "order = trace_id_sorted_sig_pos + trace_id_sorted_insig + trace_id_sorted_sig_neg\n",
    "\n",
    "\n",
    "order = order[::-1]\n",
    "line1 = len(trace_id_sorted_sig_neg)\n",
    "line2 = len(trace_id_sorted_sig_neg)+len(trace_id_sorted_insig)\n",
    "first_frame = int((len(plot_traces[0])-1)/-2)\n",
    "x_values = list(range(first_frame, first_frame + len(plot_traces)))\n",
    "\n",
    "\n",
    "custom_colorscale = [\n",
    "    [0.0, 'blue'],    # Start with white\n",
    "    [0.5, 'white'],\n",
    "    [1.0, 'red']     # End with black\n",
    "]\n",
    "\n",
    "layout = go.Layout(\n",
    "    title=(f'Sorted PMTH heatmap <br>{file_name} <br>'\n",
    "           f'pos. t: {int(100*sig_pos_count/total_count)}% ({sig_pos_count}/{total_count})   |   '\n",
    "           f'insig. t: {int(100*insig_count/total_count)}% ({insig_count}/{total_count})   |   '\n",
    "           f'neg. t: {int(100*sig_neg_count/total_count)}% ({sig_neg_count}/{total_count})'),        \n",
    "    xaxis=dict(title='Time (frame)'),\n",
    "    yaxis=dict(title='Cells (middle group: p_val > 0.05)'),\n",
    "    height=650,\n",
    "    width=750,\n",
    "    shapes=[\n",
    "        dict(type=\"line\", xref=\"paper\", yref=\"y\", x0=0, y0=line1, x1=1, y1=line1, line=dict(color=\"black\", width=2)),\n",
    "        dict(type=\"line\", xref=\"paper\", yref=\"y\", x0=0, y0=line2, x1=1, y1=line2, line=dict(color=\"black\", width=2))\n",
    "    ]\n",
    ")\n",
    "\n",
    "fig = go.Figure(layout=layout)\n",
    "\n",
    "\n",
    "# Add the heatmap trace with zmin and zmax set to -2 and 2\n",
    "fig.add_trace(go.Heatmap(\n",
    "    x=x_values, \n",
    "    z=plot_traces[order], \n",
    "    colorscale=custom_colorscale,\n",
    "    zmin=-1.96,\n",
    "    zmax=1.96\n",
    "))\n",
    "fig.show()\n",
    "\n",
    "if export_movement_heatmap:\n",
    "    fig.write_html(os.path.join(output_folder, \"movement_heatmap.html\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "avg_spikes = np.array(avg_spike_all_traces)\n",
    "\n",
    "# Find global min and max values\n",
    "global_min = np.min(avg_spikes)\n",
    "global_max = np.max(avg_spikes)\n",
    "\n",
    "# Define x-axis values\n",
    "x_values = np.linspace(-range_value, range_value, avg_spikes.shape[1])\n",
    "\n",
    "# Create subplots\n",
    "fig = make_subplots(rows=1, cols=3, subplot_titles=(\"Positive Traces\", \"Insignificant Traces\", \"Negative Traces\"))\n",
    "\n",
    "# Define plot data in the desired order\n",
    "plot_data = [\n",
    "    (trace_id_sorted_sig_pos, 'Positive'),\n",
    "    (trace_id_sorted_insig, 'Insignificant'),\n",
    "    (trace_id_sorted_sig_neg, 'Negative')\n",
    "]\n",
    "\n",
    "for i, (trace_ids, title) in enumerate(plot_data, start=1):\n",
    "    plot_spikes = avg_spikes[trace_ids]\n",
    "    \n",
    "    # Plot individual traces\n",
    "    for spike in plot_spikes:\n",
    "        fig.add_trace(go.Scatter(x=x_values, y=spike, mode='lines', line=dict(color='lightgrey', width=1), showlegend=False), row=1, col=i)\n",
    "    \n",
    "    # Plot mean trace\n",
    "    mean_line = np.mean(plot_spikes, axis=0)\n",
    "    fig.add_trace(go.Scatter(x=x_values, y=mean_line, mode='lines', line=dict(color='black', width=3), showlegend=False), row=1, col=i)\n",
    "\n",
    "# Update layout with global y-axis range\n",
    "fig.update_layout(\n",
    "    title=f'Superimposed Traces and Mean for t value groups<br>{file_name}',\n",
    "    width=1200,\n",
    "    height=400,\n",
    "    yaxis=dict(range=[global_min, global_max]),\n",
    "    yaxis2=dict(range=[global_min, global_max]),\n",
    "    yaxis3=dict(range=[global_min, global_max]),\n",
    "    xaxis=dict(range=[-range_value, range_value], title='Time (frame)'),\n",
    "    xaxis2=dict(range=[-range_value, range_value], title='Time (frame)'),\n",
    "    xaxis3=dict(range=[-range_value, range_value], title='Time (frame)')\n",
    ")\n",
    "\n",
    "# Show plot\n",
    "fig.show()\n",
    "\n",
    "fig.write_html(os.path.join(output_folder, \"pmth_average_t_groups.html\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sortedcontainers import SortedDict\n",
    "\n",
    "range_value = 104 # 104 from Cicada (int, how many steps (frames) to collect before and after the \"stimulus\")\n",
    "frames_indices = np.arange(-1 * range_value, range_value + 1)\n",
    "n_frames_psth = len(frames_indices)\n",
    "\n",
    "def get_psth_values(data, epochs_frame, low_percentile = 25, high_percentile = 75):\n",
    "\n",
    "    \n",
    "    sum_activity_by_frame_dict = SortedDict()\n",
    "\n",
    "    for epoch_index, epoch_frame in enumerate(epochs_frame):\n",
    "        beg_frame = np.max((0, epoch_frame - range_value))\n",
    "        end_frame = np.min((n_frames, epoch_frame + range_value + 1))\n",
    "\n",
    "        # before the event\n",
    "        sum_activity_before = np.sum(data[:, beg_frame:epoch_frame], axis=0) # axis 0 means cells 1, 2, 3,... are summed at each time frame.  \n",
    "        frames_before = np.arange(-(epoch_frame - beg_frame), 0)\n",
    "\n",
    "        for i, frame in enumerate(frames_before):\n",
    "            if frame not in sum_activity_by_frame_dict:\n",
    "                sum_activity_by_frame_dict[frame] = []\n",
    "            if len(sum_activity_before) > i:\n",
    "                sum_activity_by_frame_dict[frame].append(sum_activity_before[i])\n",
    "\n",
    "        # after the event\n",
    "        sum_activity_after = np.sum(data[:, epoch_frame:end_frame], axis=0)\n",
    "        frames_after = np.arange(0, end_frame - epoch_frame)\n",
    "        for i, frame in enumerate(frames_after):\n",
    "            if frame not in sum_activity_by_frame_dict:\n",
    "                sum_activity_by_frame_dict[frame] = []\n",
    "            if len(sum_activity_after) > i:\n",
    "                sum_activity_by_frame_dict[frame].append(sum_activity_after[i])\n",
    "\n",
    "\n",
    "    fcts_to_apply = [np.nanmedian, lambda x: np.nanpercentile(x, low_percentile), lambda x: np.nanpercentile(x, high_percentile)]\n",
    "    # fcts_to_apply = [np.nanmean, lambda x: np.nanpercentile(x, low_percentile), lambda x: np.nanpercentile(x, high_percentile)]\n",
    "\n",
    "\n",
    "    psth_values = list()\n",
    "    for fct_index in range(len(fcts_to_apply)):\n",
    "        psth_values.append([])\n",
    "\n",
    "    for frame in frames_indices:\n",
    "        for fct_index, fct_to_apply in enumerate(fcts_to_apply): # fcts_to_apply contains 3 functions, 1) get median, 2) get low percentile (set at 25% in cicada_psth_analysis.py), and 3) get high percentile (75%).\n",
    "            if frame not in sum_activity_by_frame_dict:\n",
    "                psth_values[fct_index].append(0)\n",
    "            else:\n",
    "                sum_activity = sum_activity_by_frame_dict[frame]\n",
    "                sum_activity = (fct_to_apply(sum_activity) / n_cells) # fct_to_apply is defined as:  np.nanpercentile(x, high_percentile)\n",
    "                psth_values[fct_index].append(sum_activity)\n",
    "\n",
    "    \n",
    "    return psth_values\n",
    "\n",
    "plot_psth_dff = get_psth_values(convert_to_dff(traces), movements_frame)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_surr_psth_values(neuronal_data,epochs_frame):\n",
    "\n",
    "    n_surrogates = 50 # 500 is default from Cicada \n",
    "    psth_frames_indices = frames_indices\n",
    "    cell_indices_in_group = np.arange(n_cells)\n",
    "    surrogate_method = \"single cell roll\"\n",
    "    verbose = False\n",
    "\n",
    "    surrogate_max_values = []\n",
    "    # Will be use to take the median, low, high percentiles of the x surrogates at each time point\n",
    "    surrogate_median_values = np.zeros((len(psth_frames_indices), n_surrogates), dtype=float)\n",
    "    surrogate_low_perc_values = np.zeros((len(psth_frames_indices), n_surrogates), dtype=float)\n",
    "    surrogate_high_perc_values = np.zeros((len(psth_frames_indices), n_surrogates), dtype=float)\n",
    "    # Loop on the n surrogates\n",
    "    for index_surrogate in range(n_surrogates):\n",
    "        surrogate_neuronal_data = np.copy(neuronal_data[cell_indices_in_group])\n",
    "        if surrogate_method == \"single cell roll\":\n",
    "            if verbose and index_surrogate == 0:\n",
    "                print(f\"Proceed to independent rolling of all cells\")\n",
    "            for cell in np.arange(len(surrogate_neuronal_data)):\n",
    "                roll_value = np.random.randint(1, surrogate_neuronal_data.shape[1])\n",
    "                surrogate_neuronal_data[cell, :] = np.roll(surrogate_neuronal_data[cell, :], shift=roll_value)\n",
    "        # if surrogate_method == \"population roll\":\n",
    "        #     if verbose and index_surrogate == 0:\n",
    "        #         print(f\"Proceed to identical rolling of all cells\")\n",
    "        #     roll_value = np.random.randint(1, surrogate_neuronal_data.shape[1])\n",
    "        #     surrogate_neuronal_data = np.roll(surrogate_neuronal_data, shift=roll_value, axis=1)\n",
    "\n",
    "        surro_psth_values = get_psth_values(data=surrogate_neuronal_data,epochs_frame=epochs_frame,low_percentile=5,high_percentile=95)\n",
    "        # Append the max values of the psth:\n",
    "        surrogate_max_values.append([np.max(surro_psth_values[0])])\n",
    "\n",
    "        # Create a matrix in which each column is the median value of a surrogate psth\n",
    "        surrogate_median_values[:, index_surrogate] = surro_psth_values[0]\n",
    "        surrogate_low_perc_values[:, index_surrogate] = surro_psth_values[1]\n",
    "        surrogate_high_perc_values[:, index_surrogate] = surro_psth_values[2]\n",
    "    # take the median at each time point for low prctile, median and high prctile of the surrogates\n",
    "    surrogate_median_psth_values = np.nanmedian(surrogate_median_values, axis=1)\n",
    "    surrogate_low_per_psth_values = np.nanmedian(surrogate_low_perc_values, axis=1)\n",
    "    surrogate_high_per_psth_values = np.nanmedian(surrogate_high_perc_values, axis=1)\n",
    "    surrogate_psth_values = [surrogate_median_psth_values, surrogate_low_per_psth_values,\n",
    "                                surrogate_high_per_psth_values]\n",
    "    \n",
    "    return surrogate_psth_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_psth_dff = get_psth_values(traces_dff, movements_frame)\n",
    "plot_psth_surr_dff = get_surr_psth_values(traces_dff, movements_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "\n",
    "psth_values = plot_psth_dff\n",
    "psth_surr_values = plot_psth_surr_dff\n",
    "\n",
    "psth_med_amplitude = round(np.max(psth_values[0])-np.median(psth_values[0]),2)\n",
    "\n",
    "# Create a figure\n",
    "fig = go.Figure()\n",
    "\n",
    "first_frame = int((len(psth_values[0])-1)/-2)\n",
    "\n",
    "# Add traces for each line\n",
    "for i, values in enumerate(psth_values):\n",
    "    if i == 0: # keep only middle line\n",
    "        x_values = list(range(first_frame, first_frame + len(values)))\n",
    "        fig.add_trace(go.Scatter(x=x_values, y=values, mode='lines', name=f'Line {i + 1}',showlegend=False))\n",
    "\n",
    "# Add traces for each line in psth_surr_values\n",
    "for i, values in enumerate(psth_surr_values):\n",
    "    if i != 0: # removing middle line\n",
    "        x_values = list(range(first_frame, first_frame + len(values)))\n",
    "        color = 'gray'\n",
    "        fig.add_trace(go.Scatter(x=x_values, y=values, mode='lines', name=f'Surrogate Line {i + 1}', line=dict(color=color), showlegend=False))\n",
    "\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title=f'PMTH for all movements with surr. lines (95%, 5%)<br>{file_name}<br>Amplitude: {psth_med_amplitude}',\n",
    "    xaxis=dict(title='Time (frame)', range=[first_frame, first_frame + len(psth_values[0]) - 1]),\n",
    "    yaxis=dict(title='%DF/F (sum of fluo. signals)'),\n",
    "    width=700,  # Set the width of the plot\n",
    "    height=450  # Set the height of the plot\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()\n",
    "\n",
    "fig.write_html(os.path.join(output_folder, \"pmth_cicada.html\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CREATE THREE SEPARATE PMTH FOR EACH MOVEMENT\n",
    "\n",
    "plot_psth_dff_movement = []\n",
    "plot_psth_surr_dff_movement = []\n",
    "\n",
    "for movement in range(n_movement_types):\n",
    "    plot_psth_dff_movement.append(get_psth_values(traces_dff, movements_frame_list[movement])) ### THIS PRODUCES 3 LISTS (movement) of 3 lists (median, low, high %)) \n",
    "    plot_psth_surr_dff_movement.append(get_surr_psth_values(traces_dff, movements_frame_list[movement]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PLOT PSTH for individual movements\n",
    "\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "# data\n",
    "psth_values = plot_psth_dff_movement.copy()\n",
    "psth_surr_values = plot_psth_surr_dff_movement.copy()\n",
    "\n",
    "subtitles = []\n",
    "for n in range(n_movement_types):\n",
    "    psth_med_amplitude = (round(np.max(psth_values[n][0]) - np.median(psth_values[n][0]), 2))\n",
    "    subtitles.append(f\"{movement_types_all[n]}<br>Amplitude: {psth_med_amplitude}\") ### DIFFERENT FROM Y MAX\n",
    "\n",
    "# Define the number of subplots and their arrangement\n",
    "rows = 1\n",
    "cols = 3\n",
    "fig = make_subplots(rows=rows, cols=cols, subplot_titles=subtitles)\n",
    "\n",
    "first_frame = int((n_frames_psth-1)/-2)\n",
    "x_values = list(range(first_frame, first_frame + n_frames_psth))\n",
    "\n",
    "max_group = [np.max(sublist[0]) for sublist in psth_values]\n",
    "min_group = [np.min(sublist[0]) for sublist in psth_values]\n",
    "y_max, y_min = np.max(max_group), np.min(min_group)\n",
    "\n",
    "# Define the number of subplots and their arrangement\n",
    "\n",
    "# Add traces for each line in each subplot\n",
    "for i in range(cols):\n",
    "    \n",
    "    fig.add_trace(go.Scatter(x=x_values, y=psth_values[i][0], mode='lines', name=f'Line {i + 1}', showlegend=False), row=1, col=i + 1)\n",
    "\n",
    "    # Add traces for psth_surr_values\n",
    "    for j, values in enumerate(psth_surr_values[i]):\n",
    "        if j != 0:  # removing middle line\n",
    "            color = 'gray'\n",
    "            fig.add_trace(go.Scatter(x=x_values, y=psth_surr_values[i][j], mode='lines', name=f'Surrogate Line {j + 1}', line=dict(color=color), showlegend=False), row=1, col=i + 1)\n",
    "\n",
    "# Update layout with shared y-axis range and x-axis values for each subplot\n",
    "    fig.update_xaxes(title='Time (frame)', range=[x_values[0], x_values[-1]], row=1, col=i + 1)\n",
    "    fig.update_yaxes(range=[y_min*1.4, y_max*1.1], row=1, col=i + 1)\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title=f'PMTH for each movement with surr. lines (5% & 95%)  ||  {file_name}',\n",
    "    title_x=0.5,\n",
    "    yaxis=dict(title='%DF/F (sum of fluo. signals)'),\n",
    "    width=1500,  # Set the total width of the plot\n",
    "    height=450,  # Set the height of the plot\n",
    "    grid=dict(rows=1, columns=3, pattern='independent'),  # Set independent grid pattern for subplots\n",
    "    margin=dict(t=100),  # Add top margin for title\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()\n",
    "fig.write_html(os.path.join(output_folder, \"pmth_sep_movements.html\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sortedcontainers import SortedDict\n",
    "\n",
    "range_value = 104 # 104 from Cicada (int, how many steps (frames) to collect before and after the \"stimulus\")\n",
    "frames_indices = np.arange(-1 * range_value, range_value + 1)\n",
    "n_frames_psth = len(frames_indices)\n",
    "\n",
    "def get_psth_sce(data, epochs_frame, low_percentile = 25, high_percentile = 75):\n",
    "\n",
    "    \n",
    "    sum_activity_by_frame_dict = SortedDict()\n",
    "\n",
    "    for epoch_index, epoch_frame in enumerate(epochs_frame):\n",
    "        beg_frame = np.max((0, epoch_frame - range_value))\n",
    "        end_frame = np.min((n_frames, epoch_frame + range_value + 1))\n",
    "\n",
    "        # before the event\n",
    "        sum_activity_before = np.sum(data[:, beg_frame:epoch_frame], axis=0) # axis 0 means cells 1, 2, 3,... are summed at each time frame.  \n",
    "        frames_before = np.arange(-(epoch_frame - beg_frame), 0)\n",
    "\n",
    "        for i, frame in enumerate(frames_before):\n",
    "            if frame not in sum_activity_by_frame_dict:\n",
    "                sum_activity_by_frame_dict[frame] = []\n",
    "            if len(sum_activity_before) > i:\n",
    "                sum_activity_by_frame_dict[frame].append(sum_activity_before[i])\n",
    "\n",
    "        # after the event\n",
    "        sum_activity_after = np.sum(data[:, epoch_frame:end_frame], axis=0)\n",
    "        frames_after = np.arange(0, end_frame - epoch_frame)\n",
    "        for i, frame in enumerate(frames_after):\n",
    "            if frame not in sum_activity_by_frame_dict:\n",
    "                sum_activity_by_frame_dict[frame] = []\n",
    "            if len(sum_activity_after) > i:\n",
    "                sum_activity_by_frame_dict[frame].append(sum_activity_after[i])\n",
    "\n",
    "\n",
    "    # fcts_to_apply = [np.nanmedian, lambda x: np.nanpercentile(x, low_percentile), lambda x: np.nanpercentile(x, high_percentile)]\n",
    "    fcts_to_apply = [np.sum] \n",
    "\n",
    "\n",
    "    psth_values = list()\n",
    "    for fct_index in range(len(fcts_to_apply)):\n",
    "        psth_values.append([])\n",
    "\n",
    "    for frame in frames_indices:\n",
    "        for fct_index, fct_to_apply in enumerate(fcts_to_apply): # fcts_to_apply contains 3 functions, 1) get median, 2) get low percentile (set at 25% in cicada_psth_analysis.py), and 3) get high percentile (75%).\n",
    "            if frame not in sum_activity_by_frame_dict:\n",
    "                psth_values[fct_index].append(0)\n",
    "            else:\n",
    "                sum_activity = sum_activity_by_frame_dict[frame]\n",
    "                sum_activity = (fct_to_apply(sum_activity)) # / n_cells) # fct_to_apply is defined as:  np.nanpercentile(x, high_percentile)\n",
    "                psth_values[fct_index].append(sum_activity)\n",
    "\n",
    "    \n",
    "    return psth_values\n",
    "\n",
    "plot_psth_sce = get_psth_sce(sce_trace, movements_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # PLOT SCE in bins\n",
    "\n",
    "# bin_in_sec = 1\n",
    "# bin_in_frames = round(bin_in_sec * frames_per_second)\n",
    "\n",
    "# psth_values = plot_psth_sce\n",
    "# summed_values = []\n",
    "# for values in psth_values:\n",
    "#     summed_values.append([sum(values[i:i+bin_in_frames]) for i in range(0, len(values), bin_in_frames)])\n",
    "\n",
    "# # Create a figure\n",
    "# fig = go.Figure()\n",
    "\n",
    "# first_frame = int((len(summed_values[0])-1)/-2)\n",
    "# print(summed_values)\n",
    "# # Add traces for each line\n",
    "# for i, values in enumerate(summed_values):\n",
    "#     x_values = list(range(first_frame, first_frame + len(values)))\n",
    "#     fig.add_trace(go.Scatter(x=x_values, y=values, mode='lines', name=f'Line {i + 1}'))\n",
    "\n",
    "# # Update layout\n",
    "# fig.update_layout(\n",
    "#     title=f'PMTH (of SCEs) for all movements with bins of {bin_in_sec} sec<br>{file_name}',\n",
    "#     xaxis=dict(title='Time (seconds)', range=[first_frame, first_frame + len(summed_values[0]) - 1]),\n",
    "#     yaxis=dict(title='SCE count'),\n",
    "#     width=700,  # Set the width of the plot\n",
    "#     height=450  # Set the height of the plot\n",
    "# )\n",
    "\n",
    "# # Show the plot\n",
    "# fig.show()\n",
    "# fig.write_html(os.path.join(output_folder, \"pmth_sce.html\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import os\n",
    "\n",
    "# bin_in_sec = 1\n",
    "# bin_in_frames = round(bin_in_sec * frames_per_second)\n",
    "bin_in_frames = 1\n",
    "\n",
    "psth_values = plot_psth_sce\n",
    "summed_values = []\n",
    "for values in psth_values:\n",
    "    summed_values.append([sum(values[i:i+bin_in_frames]) for i in range(0, len(values), bin_in_frames)])\n",
    "\n",
    "# Create a figure\n",
    "fig = go.Figure()\n",
    "\n",
    "# Adjust first_frame to be the starting frame of the x-axis\n",
    "first_frame = -(len(psth_values[0])-1)/2\n",
    "\n",
    "# Add traces for each line\n",
    "for i, values in enumerate(summed_values):\n",
    "    x_values = [first_frame + bin_in_frames * j for j in range(len(values))]\n",
    "    fig.add_trace(go.Bar(x=x_values, y=values, width=bin_in_frames, name=f'Line {i + 1}'))\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title=f'PMTH (of SCEs) for all movements with bins of 1 frame<br>{file_name}',\n",
    "    xaxis=dict(title='Time (frame)', range=[first_frame, first_frame*-1]),\n",
    "    yaxis=dict(title='SCE count'),\n",
    "    barmode='overlay',  # Bars will be overlaid instead of stacked\n",
    "    width=700,  # Set the width of the plot\n",
    "    height=450  # Set the height of the plot\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()\n",
    "fig.write_html(os.path.join(output_folder, \"pmth_sce.html\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calculate MULTIPLE ave spikes from MULTIPLE traces\n",
    "### FOCUS ON BIMODAL TRACES FROM INSIGNIFICANT GROUP\n",
    "\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "traces_to_plot = traces_dff[trace_id_sorted_insig]\n",
    "# traces_to_plot = traces_dff\n",
    "\n",
    "# traces_to_plot = traces\n",
    "\n",
    "range_value=104\n",
    "lower_percent = 2.5 \n",
    "upper_percent = 97.5\n",
    "\n",
    "mini_post_analysis_length = 5 # from center\n",
    "\n",
    "midpoint = range_value\n",
    "pre_window_start = range_value-pre_analysis_length\n",
    "mini_window_end = range_value+mini_post_analysis_length\n",
    "post_window_end = range_value+post_analysis_length\n",
    "\n",
    "avg_spike_all_traces = []\n",
    "trace_length = len(traces[0])  # Assuming all traces have the same length\n",
    "positive_trace_index = []\n",
    "neutral_trace_index = []\n",
    "negative_trace_index = []\n",
    "\n",
    "maxs = []\n",
    "mins = []\n",
    "max_frames = []\n",
    "min_frames = []\n",
    "upper_thresholds = []\n",
    "lower_thresholds = []\n",
    "\n",
    "margins_to_threshold = []\n",
    "\n",
    "pre_avgs = []\n",
    "pre_stds = []\n",
    "post_z_max = []\n",
    "post_z_min = []\n",
    "\n",
    "t_values = []\n",
    "p_values = []\n",
    "\n",
    "median_all_traces = []\n",
    "\n",
    "for trace in traces_to_plot:\n",
    "    spikes_per_trace = []\n",
    "    \n",
    "    median_all_traces.append(np.median(trace))\n",
    "\n",
    "    upper_thresholds.append(np.percentile(traces_to_plot, upper_percent))\n",
    "    lower_thresholds.append(np.percentile(traces_to_plot, lower_percent))\n",
    "\n",
    "    for frame_index in movements_frame:\n",
    "        # Calculate the start and end index for the segment\n",
    "        beg_frame = max(frame_index - range_value, 0)  # Ensure beg_frame is not negative\n",
    "        end_frame = min(frame_index + range_value + 1, trace_length)  # Ensure end_frame is within trace_length\n",
    "        \n",
    "        # Extract the segment from the time series and pad with zeros if necessary\n",
    "        segment = trace[beg_frame:end_frame]\n",
    "\n",
    "        if len(segment) < 2 * range_value + 1:\n",
    "            pad_length = 2 * range_value + 1 - len(segment)\n",
    "            segment = list(segment)\n",
    "            segment.extend([0] * pad_length)\n",
    "            segment = np.array(segment)          \n",
    "        \n",
    "        spikes_per_trace.append(segment)\n",
    "\n",
    "    # print(len(segments))\n",
    "    # Compute the average of the segments for the current trace\n",
    "    avg_spike_per_trace = [sum(values) / len(spikes_per_trace) for values in zip(*spikes_per_trace)]\n",
    "    avg_spike_all_traces.append(avg_spike_per_trace)\n",
    "\n",
    "    ### here we're using a mini window after zero  \n",
    "    pre_avg_spike_per_trace = avg_spike_per_trace[midpoint:mini_window_end] ### WHOLE WINDOW (NOT USING pre_analysis_length)\n",
    "    post_avg_spike_per_trace = avg_spike_per_trace[mini_window_end:post_window_end]\n",
    "\n",
    "    t_stat, p_value = ttest_ind(post_avg_spike_per_trace, pre_avg_spike_per_trace)\n",
    "    t_values.append(t_stat)\n",
    "    p_values.append(p_value)\n",
    "\n",
    "    ### looking at negative and positive peaks only\n",
    "    max_value = np.max(avg_spike_per_trace[midpoint:post_window_end])\n",
    "    min_value = np.min(avg_spike_per_trace[midpoint:post_window_end])\n",
    "    maxs.append(max_value)\n",
    "    mins.append(min_value)\n",
    "\n",
    "    max_index = np.argmax(avg_spike_per_trace[midpoint:post_window_end])\n",
    "    min_index = np.argmin(avg_spike_per_trace[midpoint:post_window_end])\n",
    "    max_frames.append(max_index)\n",
    "    min_frames.append(min_index)\n",
    "\n",
    "insig_mean_line = np.mean(plot_spikes, axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### SORTING BY REFOCUSING ON INSIGNIFICANT TRACES (not ARTEM's version because it compares the peak to the premovement average, whereas here we're comparing two peaks) \n",
    "\n",
    "threshold_value = 0.05 # for p values\n",
    "\n",
    "# Get indices of values above and below 0.05\n",
    "trace_id_sig = [i for i, p_val in enumerate(p_values) if p_val <= threshold_value]\n",
    "trace_id_insig = [i for i, p_val in enumerate(p_values) if p_val > threshold_value]\n",
    "\n",
    "# Sort only certain indices based on the condition\n",
    "trace_id_sorted_sig = sorted(trace_id_sig, key=lambda idx: t_values[idx], reverse=True)\n",
    "trace_id_sorted_insig = sorted(trace_id_insig, key=lambda idx: t_values[idx], reverse=True)\n",
    "\n",
    "trace_id_sorted_sig_pos = []\n",
    "trace_id_sorted_sig_neg = []\n",
    "\n",
    "for i, idx in enumerate(trace_id_sorted_sig):\n",
    "    if t_values[idx] >= 0:\n",
    "        trace_id_sorted_sig_pos.append(idx)\n",
    "        # print('pos ', i, ' ', t_values[idx])\n",
    "    else:\n",
    "        trace_id_sorted_sig_neg.append(idx)\n",
    "        # print('neg ',i,' ', t_values[idx])\n",
    "\n",
    "print(len(trace_id_sorted_sig_pos), ' ', len(trace_id_sorted_insig), ' ',len(trace_id_sorted_sig_neg) )\n",
    "print(len(trace_id_sorted_sig_pos)+len(trace_id_sorted_sig_neg)+len(trace_id_sorted_insig))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### SORTING BY MIN MAX SLOPES (not ARTEM's version because it compares the peak to the premovement average, whereas here we're comparing two peaks) \n",
    "\n",
    "threshold_value = 0.05 # for p values\n",
    "\n",
    "# Get indices of values above and below 0.05\n",
    "# Initialize empty lists\n",
    "trace_id_pos = []\n",
    "trace_id_neg = []\n",
    "trace_id_bi = []\n",
    "trace_id_insig = []\n",
    "\n",
    "# Single loop for enumeration and conditional logic\n",
    "for i, (max_val, min_val, upper_threshold, lower_threshold, max_frame, min_frame) in enumerate(zip(maxs, mins,upper_thresholds,lower_thresholds, max_frames, min_frames)):\n",
    "    if max_val >= upper_threshold and min_val >= lower_threshold:\n",
    "        trace_id_pos.append(i)\n",
    "    elif max_val <= upper_threshold and min_val <= lower_threshold:\n",
    "        trace_id_neg.append(i)\n",
    "    elif max_val >= upper_threshold and min_val < lower_threshold and min_frame > max_frame:\n",
    "        trace_id_bi.append(i)\n",
    "    elif max_val < upper_threshold and min_val >= lower_threshold:\n",
    "        trace_id_insig.append(i)\n",
    "\n",
    "n_trace_id_pos = len(trace_id_pos)\n",
    "n_trace_id_neg = len(trace_id_neg)\n",
    "n_trace_id_bi = len(trace_id_bi)\n",
    "n_trace_id_insig =len(trace_id_insig)\n",
    "print(f\"check all traces captured: {n_trace_id_pos} {n_trace_id_neg} {n_trace_id_bi} {n_trace_id_insig}\")\n",
    "\n",
    "print(f\"check all traces captured: {n_cells==n_trace_id_pos+n_trace_id_neg+n_trace_id_bi+n_trace_id_insig}\")\n",
    "\n",
    "\n",
    "# # Sort only certain indices based on the condition\n",
    "# trace_id_sorted_sig = sorted(trace_id_sig, key=lambda idx: t_values[idx], reverse=True)\n",
    "# trace_id_sorted_insig = sorted(trace_id_insig, key=lambda idx: t_values[idx], reverse=True)\n",
    "\n",
    "# trace_id_sorted_sig_pos = []\n",
    "# trace_id_sorted_sig_neg = []\n",
    "\n",
    "# for i, idx in enumerate(trace_id_sorted_sig):\n",
    "#     if t_values[idx] >= 0:\n",
    "#         trace_id_sorted_sig_pos.append(idx)\n",
    "#         # print('pos ', i, ' ', t_values[idx])\n",
    "#     else:\n",
    "#         trace_id_sorted_sig_neg.append(idx)\n",
    "#         # print('neg ',i,' ', t_values[idx])\n",
    "\n",
    "# print(len(trace_id_sorted_sig_pos), ' ', len(trace_id_sorted_insig), ' ',len(trace_id_sorted_sig_neg) )\n",
    "# print(len(trace_id_sorted_sig_pos)+len(trace_id_sorted_sig_neg)+len(trace_id_sorted_insig))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "\n",
    "export_movement_heatmap = True\n",
    "\n",
    "sig_pos_count = len(trace_id_sorted_sig_pos)\n",
    "sig_neg_count = len(trace_id_sorted_sig_neg)\n",
    "insig_count = len(trace_id_sorted_insig)\n",
    "\n",
    "total_count = sig_pos_count + sig_neg_count + insig_count\n",
    "\n",
    "# plot_traces = np.array(avg_spike_all_traces)\n",
    "plot_traces = stats.zscore(avg_spike_all_traces, axis=1)\n",
    "\n",
    "order = trace_id_sorted_sig_pos + trace_id_sorted_insig + trace_id_sorted_sig_neg\n",
    "\n",
    "\n",
    "order = order[::-1]\n",
    "line1 = len(trace_id_sorted_sig_neg)\n",
    "line2 = len(trace_id_sorted_sig_neg)+len(trace_id_sorted_insig)\n",
    "first_frame = int((len(plot_traces[0])-1)/-2)\n",
    "x_values = list(range(first_frame, first_frame + len(plot_traces)))\n",
    "\n",
    "\n",
    "custom_colorscale = [\n",
    "    [0.0, 'blue'],    # Start with white\n",
    "    [0.5, 'white'],\n",
    "    [1.0, 'red']     # End with black\n",
    "]\n",
    "\n",
    "layout = go.Layout(\n",
    "    title=(f'Sorted PMTH heatmap of INSIGNIFICANT TRACES<br>{file_name} <br>'\n",
    "           f'pos. t: {int(100*sig_pos_count/total_count)}% ({sig_pos_count}/{total_count})   |   '\n",
    "           f'insig. t: {int(100*insig_count/total_count)}% ({insig_count}/{total_count})   |   '\n",
    "           f'neg. t: {int(100*sig_neg_count/total_count)}% ({sig_neg_count}/{total_count})'),        \n",
    "    xaxis=dict(title='Time (frame)'),\n",
    "    yaxis=dict(title='Cells (middle group: p_val > 0.05)'),\n",
    "    height=650,\n",
    "    width=750,\n",
    "    shapes=[\n",
    "        dict(type=\"line\", xref=\"paper\", yref=\"y\", x0=0, y0=line1, x1=1, y1=line1, line=dict(color=\"black\", width=2)),\n",
    "        dict(type=\"line\", xref=\"paper\", yref=\"y\", x0=0, y0=line2, x1=1, y1=line2, line=dict(color=\"black\", width=2))\n",
    "    ]\n",
    ")\n",
    "\n",
    "fig = go.Figure(layout=layout)\n",
    "\n",
    "\n",
    "# Add the heatmap trace with zmin and zmax set to -2 and 2\n",
    "fig.add_trace(go.Heatmap(\n",
    "    x=x_values, \n",
    "    z=plot_traces[order], \n",
    "    colorscale=custom_colorscale,\n",
    "    zmin=-1.96,\n",
    "    zmax=1.96\n",
    "))\n",
    "fig.show()\n",
    "\n",
    "if export_movement_heatmap:\n",
    "    fig.write_html(os.path.join(output_folder, \"movement_heatmap_insig_only.html\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "avg_spikes = np.array(avg_spike_all_traces)\n",
    "\n",
    "# Find global min and max values\n",
    "global_min = np.min(avg_spikes)\n",
    "global_max = np.max(avg_spikes)\n",
    "\n",
    "# Define x-axis values\n",
    "x_values = np.linspace(-range_value, range_value, avg_spikes.shape[1])\n",
    "\n",
    "# Create subplots\n",
    "fig = make_subplots(rows=1, cols=3, subplot_titles=(\"Positive Traces\", \"Insignificant Traces\", \"Negative Traces\"))\n",
    "\n",
    "# Define plot data in the desired order\n",
    "plot_data = [\n",
    "    (trace_id_sorted_sig_pos, 'Positive'),\n",
    "    (trace_id_sorted_insig, 'Insignificant'),\n",
    "    (trace_id_sorted_sig_neg, 'Negative')\n",
    "]\n",
    "\n",
    "for i, (trace_ids, title) in enumerate(plot_data, start=1):\n",
    "    plot_spikes = avg_spikes[trace_ids]\n",
    "    \n",
    "    # Plot individual traces\n",
    "    for spike in plot_spikes:\n",
    "        fig.add_trace(go.Scatter(x=x_values, y=spike, mode='lines', line=dict(color='lightgrey', width=1), showlegend=False), row=1, col=i)\n",
    "    \n",
    "    # Plot mean trace\n",
    "    mean_line = np.mean(plot_spikes, axis=0)\n",
    "    fig.add_trace(go.Scatter(x=x_values, y=mean_line, mode='lines', line=dict(color='black', width=3), showlegend=False), row=1, col=i)\n",
    "\n",
    "# Update layout with global y-axis range\n",
    "fig.update_layout(\n",
    "    title=f'Superimposed Traces and Mean for t value groups of INSIGNIFICANT TRACES<br>{file_name}',\n",
    "    width=1200,\n",
    "    height=400,\n",
    "    yaxis=dict(range=[global_min, global_max]),\n",
    "    yaxis2=dict(range=[global_min, global_max]),\n",
    "    yaxis3=dict(range=[global_min, global_max]),\n",
    "    xaxis=dict(range=[-range_value, range_value], title='Time (frame)'),\n",
    "    xaxis2=dict(range=[-range_value, range_value], title='Time (frame)'),\n",
    "    xaxis3=dict(range=[-range_value, range_value], title='Time (frame)')\n",
    ")\n",
    "\n",
    "# Show plot\n",
    "fig.show()\n",
    "\n",
    "fig.write_html(os.path.join(output_folder, \"pmth_average_t_groups_insig_only.html\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import random\n",
    "from scipy.signal import savgol_filter\n",
    "\n",
    "# Sample list of time series (each time series is a numpy array)\n",
    "# Pick a random time series\n",
    "# ts = random.choice(avg_spike_all_traces)\n",
    "\n",
    "insig_mean_line = np.mean(plot_spikes, axis=0)\n",
    "# plt.plot(insig_mean_line)\n",
    "\n",
    "ts = insig_mean_line\n",
    "\n",
    "# trace_id = avg_spike_all_traces.index(ts)\n",
    "trace_id = 0\n",
    "\n",
    "time = np.arange(len(ts))\n",
    "\n",
    "# Define the time frame\n",
    "start, end = range_value, range_value + post_analysis_length\n",
    "\n",
    "# Find the max and min values within the specified time frame\n",
    "max_val_idx = np.argmax(ts[start:end + 1]) + start\n",
    "min_val_idx = np.argmin(ts[start:end + 1]) + start\n",
    "\n",
    "# Apply Savitzky-Golay filter for smoothing\n",
    "smoothed_ts = savgol_filter(ts, window_length=20, polyorder=2) #11\n",
    "# smoothed_ts = savgol_filter(ts, window_length=7, polyorder=3) ### from JC\n",
    "\n",
    "\n",
    "# Compute the first derivative\n",
    "first_derivative = np.diff(smoothed_ts, prepend=ts[0])\n",
    "# first_derivative = np.diff(ts, prepend=ts[0])\n",
    "\n",
    "# Calculate the median of the original time series\n",
    "median_value = median_all_traces[trace_id]\n",
    "\n",
    "# Create the plot\n",
    "fig = go.Figure()\n",
    "\n",
    "# Original time series\n",
    "fig.add_trace(go.Scatter(x=time, y=ts, mode='lines', line=dict(color='gray'), name=f'Time Series (Trace ID: {trace_id})'))\n",
    "\n",
    "# Smoothed time series\n",
    "fig.add_trace(go.Scatter(x=time, y=smoothed_ts, mode='lines', line=dict(color='black'), name='Smoothed Time Series'))\n",
    "\n",
    "# First derivative\n",
    "fig.add_trace(go.Scatter(x=time, y=first_derivative, mode='lines', line=dict(color='red'), name='First Derivative of smoothed'))\n",
    "\n",
    "# Max and min values within the specified time frame\n",
    "fig.add_trace(go.Scatter(x=[time[max_val_idx]], y=[ts[max_val_idx]], mode='markers', marker=dict(color='green', size=10), name='Max Value'))\n",
    "fig.add_trace(go.Scatter(x=[time[min_val_idx]], y=[ts[min_val_idx]], mode='markers', marker=dict(color='blue', size=10), name='Min Value'))\n",
    "\n",
    "# Highlight the time frame\n",
    "fig.add_vline(x=start, line=dict(color='grey', dash='dash'))\n",
    "fig.add_vline(x=end, line=dict(color='grey', dash='dash'))\n",
    "\n",
    "# Add the horizontal median line\n",
    "fig.add_hline(y=median_value, line=dict(color='purple', dash='dash'), name='Median Value')\n",
    "\n",
    "# Add labels and title\n",
    "fig.update_layout(title='Average of all traces + first derivative',\n",
    "                  xaxis_title='Time',\n",
    "                  yaxis_title='Value',\n",
    "                  legend_title='Legend',\n",
    "                  width=1000,  # Set the width of the plot\n",
    "                  height=600)  # Set the height of the plot\n",
    "\n",
    "# Display the plot\n",
    "fig.show()\n",
    "\n",
    "fig.write_html(os.path.join(output_folder, \"pmth_average_insig_derivative.html\"))\n",
    "# 108about:blank#blocked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def save_variables_to_txt(filename, **kwargs):\n",
    "#     with open(filename, 'w') as f:\n",
    "#         for key, value in kwargs.items():\n",
    "#             f.write(f\"{key}: {value}\\n\")\n",
    "\n",
    "# # Example usage\n",
    "# save_variables_to_txt(os.path.join(suite2p_folder, \"export_data\"), \n",
    "#                       sce_count=n_sce, \n",
    "#                       sce_per_minute=sce_per_min,\n",
    "#                       sce_threshold=sce_n_cells_threshold,\n",
    "#                       spikes_count=n_spikes,\n",
    "#                       spikes_per_min=spikes_per_min, \n",
    "#                       spikes_per_min_per_cell = spikes_per_min_per_cell,\n",
    "#                       movement_count=movement_count,\n",
    "#                       movement_per_min=movement_per_min)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openpyxl import load_workbook\n",
    "\n",
    "# Load your workbook and select the sheet\n",
    "workbook = load_workbook(main_excel_path)\n",
    "sheet = workbook.active\n",
    "\n",
    "# Define the column names you want to use starting from column 3\n",
    "column_names = [\n",
    "    'sce_n_cells_threshold', 'sce_count', 'sce_per_sec', 'spikes_count', 'spikes_per_sec', \n",
    "    'spikes_per_sec_per_cell', 'movement_count', 'movement_per_min', 'sig_pos_count', \n",
    "    'insig_count', 'sig_neg_count'\n",
    "]\n",
    "\n",
    "# Create the column indices starting from column 3\n",
    "column_indices = {name: i+4 for i, name in enumerate(column_names)}\n",
    "\n",
    "# Write the column names into the header row starting from column 3\n",
    "for name, col in column_indices.items():\n",
    "    sheet.cell(row=1, column=col, value=name)\n",
    "\n",
    "# Insert the values into the specified cells\n",
    "sheet.cell(row=excel_row_id + 2, column=column_indices['sce_n_cells_threshold'], value=sce_n_cells_threshold)\n",
    "sheet.cell(row=excel_row_id + 2, column=column_indices['sce_count'], value=n_sce)\n",
    "sheet.cell(row=excel_row_id + 2, column=column_indices['sce_per_sec'], value=sce_per_sec)\n",
    "sheet.cell(row=excel_row_id + 2, column=column_indices['spikes_count'], value=n_spikes)\n",
    "sheet.cell(row=excel_row_id + 2, column=column_indices['spikes_per_sec'], value=spikes_per_sec)\n",
    "sheet.cell(row=excel_row_id + 2, column=column_indices['spikes_per_sec_per_cell'], value=spikes_per_sec_per_cell)\n",
    "sheet.cell(row=excel_row_id + 2, column=column_indices['movement_count'], value=movement_count)\n",
    "sheet.cell(row=excel_row_id + 2, column=column_indices['movement_per_min'], value=movement_per_min)\n",
    "\n",
    "sheet.cell(row=excel_row_id + 2, column=column_indices['sig_pos_count'], value=len(trace_id_sorted_sig_pos))\n",
    "sheet.cell(row=excel_row_id + 2, column=column_indices['insig_count'], value=len(trace_id_sorted_insig))\n",
    "sheet.cell(row=excel_row_id + 2, column=column_indices['sig_neg_count'], value=len(trace_id_sorted_sig_neg))\n",
    "\n",
    "# Save the workbook\n",
    "workbook.save(main_excel_path)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
